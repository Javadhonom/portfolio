{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b125ac61",
   "metadata": {},
   "source": [
    "## initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7e8804",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d842d9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from initailFunctionsPath import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469fdf04",
   "metadata": {},
   "source": [
    "### Spark instaniation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e36e89f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "(conf\n",
    ".set('spark.driver.memory', '140g')\n",
    ".set('spark.executer.cores', '58')\n",
    ".set('spark.shuffle.service.index.cache.size', '3g')\n",
    ".setAppName('Practice') )\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c797ab",
   "metadata": {},
   "source": [
    "## data inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dab27c",
   "metadata": {},
   "source": [
    "### load daily price and shrout data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a00fd371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1055114\n",
      "+--------+------+-----------+--------------------+------+---------+\n",
      "|date    |symbol|close_price|close_price_adjusted|shrout|mktcap   |\n",
      "+--------+------+-----------+--------------------+------+---------+\n",
      "|13980221|آ س پ |1366.0     |1249.0              |9.0E8 |122940.0 |\n",
      "|13990624|آ س پ |15262.0    |15127.0             |1.0E9 |1526200.0|\n",
      "|13990904|آ س پ |11771.0    |11667.0             |1.0E9 |1177100.0|\n",
      "+--------+------+-----------+--------------------+------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "price_df = (\n",
    "    spark.read.parquet(PRICE_PATH + '/Cleaned_Stock_Prices_14001127.parquet')\n",
    "    .filter(F.col('jalaliDate').between(MIN_ANALYSIS_DATE, MAX_ANALYSIS_DATE))\n",
    "    .select(\n",
    "        F.col('jalaliDate').alias('date'),\n",
    "        F.col('name').alias('symbol'),\n",
    "        'close_price',\n",
    "        'close_price_adjusted',\n",
    "        'shrout',\n",
    "        (F.col('MarketCap') / 10**7).alias('mktcap')\n",
    "    )\n",
    "    .dropDuplicates()\n",
    ")\n",
    "\n",
    "display_df(price_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f9068e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|min_date|max_date|\n",
      "+--------+--------+\n",
      "|13980105|14001127|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MIN_PRICE_DATE = price_df.agg(F.min('date')).collect()[0][0]\n",
    "MAX_PRICE_DATE = price_df.agg(F.max('date')).collect()[0][0]\n",
    "min_max(price_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4f7b92",
   "metadata": {},
   "source": [
    "### load valid symbols data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05f80d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1279\n",
      "+------+\n",
      "|symbol|\n",
      "+------+\n",
      "|ختوقا |\n",
      "|همراه |\n",
      "|بالاس |\n",
      "+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "valid_symbols_df = (\n",
    "    spark.read.parquet(VALID_SYMBOLS_PATH + '/validSymbols.parquet')\n",
    ")\n",
    "\n",
    "display_df(valid_symbols_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4de6721",
   "metadata": {},
   "source": [
    "### load daily trade data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f685558c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trade_df = (\n",
    "#     spark.read.parquet(PATH_PORTFOLIO + '/trade_df.parquet')\n",
    "#     .join(valid_symbols_df, on = ['symbol'], how = 'inner')\n",
    "# )\n",
    "\n",
    "# display_df(trade_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "770cc438",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print('missing nTradeShares: ', round(trade_df.filter(F.col('nTradeShares') == 0).count() / trade_df.count(), 5))\n",
    "# print('missing tradeSettlementValue: ', round(trade_df.filter(F.col('tradeSettlementValue') == 0).count() / trade_df.count(), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11714f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\n",
    "#     trade_df\n",
    "#     .agg(F.max('date').alias('maxDate'))\n",
    "#     .show()\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc780728",
   "metadata": {},
   "source": [
    "### load initial portfolio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9740e5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# portfolio_df = (\n",
    "#     spark.read.parquet(PATH_PORTFOLIO + '/portfolio_df.parquet')\n",
    "#     .join(valid_symbols_df, on = ['symbol'], how = 'inner')\n",
    "# )\n",
    "\n",
    "# display_df(portfolio_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7045604a",
   "metadata": {},
   "source": [
    "### load daily portfolio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87f3758a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "729153558\n",
      "+------+--------+---------+------------------+----------+----------+\n",
      "|symbol|date    |accountId|heldShares        |netCashOut|netCashIn |\n",
      "+------+--------+---------+------------------+----------+----------+\n",
      "|سیتا  |13990411|2        |333.1922995557436 |0.0       |-0.4725   |\n",
      "|شاروم |13990515|2        |15.992553731595871|0.0       |-0.00945  |\n",
      "|وکبهمن|13990919|11       |58.323434241748764|0.0       |-0.0811104|\n",
      "+------+--------+---------+------------------+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "daily_portfolio_df = (\n",
    "    spark.read.parquet(PATH_PORTFOLIO + '/daily_portfolio_df.parquet')\n",
    "    .join(valid_symbols_df, on = ['symbol'], how = 'inner')\n",
    ")\n",
    "\n",
    "display_df(daily_portfolio_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc181958",
   "metadata": {},
   "source": [
    "### load adjusted initial portfolio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d36f8a51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8859415\n",
      "+------+--------+---------+-----------------+\n",
      "|symbol|date    |accountId|nHeldShares      |\n",
      "+------+--------+---------+-----------------+\n",
      "|بزاگرس|13980105|9817865  |925.1959619952494|\n",
      "|بزاگرس|13980105|13793671 |925.1959619952494|\n",
      "|بزاگرس|13980105|3732476  |927.2565320665083|\n",
      "+------+--------+---------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adjusted_portfolio_df = (\n",
    "    spark.read.parquet(PATH_PORTFOLIO + '/adjusted_portfolio_df.parquet')\n",
    "    .join(valid_symbols_df, on = ['symbol'], how = 'inner')\n",
    ")\n",
    "\n",
    "display_df(adjusted_portfolio_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42090b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why count(adjusted_portfolio_df) > count(portfolio_df)?\n",
    "\n",
    "# (\n",
    "#     portfolio_df\n",
    "#     .join(adjusted_portfolio_df, on = ['symbol', 'date', 'accountId'], how = 'left_anti')\n",
    "#     .count()\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456ebc93",
   "metadata": {},
   "source": [
    "### load invalid holdings data (negative number of shares!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b93f66f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70544770\n",
      "+------+---------+--------------+\n",
      "|symbol|accountId|invalidHolding|\n",
      "+------+---------+--------------+\n",
      "|اپال  |5        |1             |\n",
      "|سبزوا |17       |1             |\n",
      "|کویر  |28       |1             |\n",
      "+------+---------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# accountIds with negative holdings of a stock?\n",
    "\n",
    "invalid_holdings_df = (\n",
    "    spark.read.parquet(PATH_PORTFOLIO + '/invalid_holdings_df.parquet')\n",
    "    .join(valid_symbols_df, on = ['symbol'], how = 'inner')\n",
    ")\n",
    "\n",
    "display_df(invalid_holdings_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0214b7",
   "metadata": {},
   "source": [
    "### load flat daily trade data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f22a99f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# flat_trade_df = (\n",
    "#     spark.read.parquet(PATH_PORTFOLIO + '/flat_trade_df.parquet')\n",
    "#     .join(valid_symbols_df, on = ['symbol'], how = 'inner')\n",
    "# )\n",
    "\n",
    "# display_df(flat_trade_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b9a8b5",
   "metadata": {},
   "source": [
    "### load adjusted flat trade data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62951cd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1829163066\n",
      "+------+--------+---------+-----------------+---------------+-------+------+\n",
      "|symbol|date    |accountId|nTradeShares     |settlementValue|cashOut|cashIn|\n",
      "+------+--------+---------+-----------------+---------------+-------+------+\n",
      "|تبرک  |13980105|93649    |5461.316674675636|-1.12          |0.0    |-1.12 |\n",
      "|تبرک  |13980105|93649    |5461.316674675636|-1.121         |0.0    |-1.121|\n",
      "|تبرک  |13980105|93649    |5461.316674675636|-1.125         |0.0    |-1.125|\n",
      "+------+--------+---------+-----------------+---------------+-------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adjusted_raw_flat_trade_df = (\n",
    "    spark.read.parquet(PATH_PORTFOLIO + '/adjusted_raw_flat_trade_df.parquet')\n",
    "    .join(valid_symbols_df, on = ['symbol'], how = 'inner')\n",
    ")\n",
    "\n",
    "display_df(adjusted_raw_flat_trade_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "177142f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wwhy count(adjusted_raw_flat_trade_df) > count(flat_trade_df)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfee1846",
   "metadata": {},
   "source": [
    "## data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f89a234",
   "metadata": {},
   "source": [
    "### find new entrants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f286fd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_entrant_account_ids_df = (\n",
    "#     flat_trade_df\n",
    "#     .groupBy('accountId')\n",
    "#     .agg(\n",
    "#         F.min('date').alias('firstDate')\n",
    "#     )\n",
    "#     .join(portfolio_df.select('accountId').distinct(), on = ['accountId'], how = 'left_anti')\n",
    "# )\n",
    "\n",
    "# display_df(new_entrant_account_ids_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcd04da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/Administrator/Heidari_Ra/Outputs/'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d0ea14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\n",
    "#     new_entrant_account_ids_df\n",
    "#     .write.mode('overwrite').parquet(PATH_OUTPUT + '{}'.format('/new_entrant_account_ids.parquet'))\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bea8eeb",
   "metadata": {},
   "source": [
    "### time series of new entrants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84c9b19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_entrants_time_series_df = (\n",
    "#     flat_trade_df\n",
    "#     .select('date', 'accountId')\n",
    "#     .dropDuplicates()\n",
    "#     .join(new_entrant_account_ids_df, on = 'accountId', how = 'inner')\n",
    "#     .withColumn('rank', F.row_number().over(Window.partitionBy('accountId').orderBy('date')))\n",
    "#     .filter(F.col('rank') == 1)\n",
    "#     .drop('rank')\n",
    "#     .groupBy('date')\n",
    "#     .count()\n",
    "#     .orderBy('date')\n",
    "# )\n",
    "\n",
    "# display_df(new_entrants_time_series_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b0d66e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\n",
    "#     new_entrants_time_series_df\n",
    "#     .write.mode('overwrite').parquet(PATH_OUTPUT + '/new_entrantd_time_series_df.parquet')\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8921a7",
   "metadata": {},
   "source": [
    "### calculate gain from trade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd50b51f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10733148\n",
      "+---------+-----------------+-------------------+\n",
      "|accountId|netCashOut       |netCashIn          |\n",
      "+---------+-----------------+-------------------+\n",
      "|9460558  |86.83102269999999|-92.51729630000001 |\n",
      "|2088053  |19287.23959359999|-18874.875769300008|\n",
      "|7625478  |6092.705459899998|-6088.591398299999 |\n",
      "+---------+-----------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gain_from_trade_df = (\n",
    "    adjusted_raw_flat_trade_df\n",
    "    .groupBy('accountId')\n",
    "    .agg(\n",
    "        F.sum('cashOut').alias('netCashOut'),\n",
    "        F.sum('cashIn').alias('netCashIn')\n",
    "    )\n",
    ")\n",
    "\n",
    "display_df(gain_from_trade_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9d0b98",
   "metadata": {},
   "source": [
    "### calculate value of the initial portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99972117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4146519\n",
      "+---------+---------------------+\n",
      "|accountId|initialPortfolioValue|\n",
      "+---------+---------------------+\n",
      "|50219    |0.5334175            |\n",
      "|81085    |51.522831599999996   |\n",
      "|133042   |0.045924000000000006 |\n",
      "+---------+---------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "initial_portfolio_value_df = (\n",
    "    adjusted_portfolio_df\n",
    "    .join(price_df.select('date', 'symbol', 'close_price_adjusted'), on = ['date', 'symbol'], how = 'left')\n",
    "    .withColumnRenamed('close_price_adjusted','close_price')\n",
    "    .dropna(subset = ['close_price'])\n",
    "    .join(invalid_holdings_df, on = ['accountId', 'symbol'], how = 'left')\n",
    "    .filter(F.col('invalidHolding').isNull())\n",
    "    .withColumn('value', F.col('nHeldShares') * F.col('close_price'))\n",
    "    .groupBy('accountId')\n",
    "    .agg(\n",
    "        (F.sum('value') / 10**7).alias('initialPortfolioValue')\n",
    "    )\n",
    ")\n",
    "\n",
    "display_df(initial_portfolio_value_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "471edd59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(initial_portfolio_value_df.filter(F.col('initialPortfolioValue').isNull()).count())\n",
    "print(initial_portfolio_value_df.filter(F.col('initialPortfolioValue') <= 0).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b160c9ac",
   "metadata": {},
   "source": [
    "### calculate value of the final portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37d21e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12472786\n",
      "+---------+-------------------+\n",
      "|accountId|finalPortfolioValue|\n",
      "+---------+-------------------+\n",
      "|3305619  |55.282747828171026 |\n",
      "|3873383  |27.090303378168368 |\n",
      "|3914262  |27.081233002856596 |\n",
      "+---------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_portfolio_value_df = (\n",
    "    daily_portfolio_df\n",
    "    .withColumn('rowNumber', F.row_number().over(Window.partitionBy('accountId', 'symbol').orderBy('date')))\n",
    "    .withColumn('maxRowNumber', F.max('rowNumber').over(Window.partitionBy('accountId', 'symbol')))\n",
    "    .filter(F.col('rowNumber') == F.col('maxRowNumber'))\n",
    "    .filter(F.col('heldShares') > 0)\n",
    "    .withColumn('date', F.lit(MAX_PRICE_DATE))\n",
    "    .join(price_df.select('date', 'symbol', 'close_price_adjusted'), on = ['date', 'symbol'], how = 'left')\n",
    "    .withColumnRenamed('close_price_adjusted','close_price')\n",
    "    .dropna(subset = ['close_price'])\n",
    "    .withColumn('value', F.col('heldShares') * F.col('close_price'))\n",
    "    .groupBy('accountId')\n",
    "    .agg(\n",
    "        (F.sum('value') / 10**7).alias('finalPortfolioValue')\n",
    "    )   \n",
    ")\n",
    "\n",
    "display_df(final_portfolio_value_df)\n",
    "# count after join?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "189eaef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(final_portfolio_value_df.filter(F.col('finalPortfolioValue').isNull()).count())\n",
    "print(final_portfolio_value_df.filter(F.col('finalPortfolioValue') <= 0).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7105f5b5",
   "metadata": {},
   "source": [
    "### symbols in with-one-symbol portfolios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45409fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277399243\n",
      "+------+--------+---------+------------------+----------+---------+--------------+\n",
      "|symbol|date    |accountId|heldShares        |netCashOut|netCashIn|numberOfStocks|\n",
      "+------+--------+---------+------------------+----------+---------+--------------+\n",
      "|وسپهر |13990702|26       |1222.8704010606561|0.0       |-1.111   |2             |\n",
      "|سکارون|13980105|26       |864.5322381930185 |0.0       |0.0      |2             |\n",
      "|وسپهر |13990702|29       |1133.9343718926086|0.0       |-1.0302  |10            |\n",
      "+------+--------+---------+------------------+----------+---------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stock_count_within_portfolio_df = ( \n",
    "    daily_portfolio_df\n",
    "    .dropDuplicates(subset= ['accountId','symbol'])\n",
    "    .withColumn('numberOfStocks', F.row_number().over(Window.partitionBy('accountId').orderBy(F.desc('heldShares'))))\n",
    "    .withColumn('numberOfStocks', F.max('numberOfStocks').over(Window.partitionBy('accountId')))\n",
    ")\n",
    "\n",
    "display_df(stock_count_within_portfolio_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25a67bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "897\n",
      "+------+--------------+\n",
      "|symbol|accountNumbers|\n",
      "+------+--------------+\n",
      "|سمایه |157888        |\n",
      "|وتوصا |136985        |\n",
      "|دی    |96943         |\n",
      "+------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "symbols_in_one_symbol_portfolios_df = (\n",
    "    stock_count_within_portfolio_df\n",
    "    .filter(F.col('numberOfStocks') == 1)\n",
    "    .withColumn('accountNumbers', F.row_number().over(Window.partitionBy('symbol').orderBy(F.desc('heldShares'))))\n",
    "    .withColumn('accountNumbers', F.max('accountNumbers').over(Window.partitionBy('symbol')))\n",
    "    .dropDuplicates(subset = ['symbol'])\n",
    "    .select(\n",
    "        F.col('symbol'),\n",
    "        F.col('accountNumbers'),\n",
    "    )\n",
    "    .orderBy(F.desc('accountNumbers'))\n",
    ")\n",
    "\n",
    "display_df(symbols_in_one_symbol_portfolios_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dcf2b533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "897\n",
      "+------+--------------+\n",
      "|symbol|accountNumbers|\n",
      "+------+--------------+\n",
      "|سمایه |157888        |\n",
      "|وتوصا |136985        |\n",
      "|دی    |96943         |\n",
      "+------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_df(symbols_in_one_symbol_portfolios_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8876ec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    symbols_in_one_symbol_portfolios_df\n",
    "    .write.mode('overwrite').parquet(r\"C:\\Users\\Administrator\\Heidari_Ra\\Outputs\\\\\" + 'symbols_in_one_symbol_portfolios_df.parquet')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4559c87",
   "metadata": {},
   "source": [
    "### time series of the net cash-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9e5958a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12490421\n",
      "+---------+------------+\n",
      "|accountId|type        |\n",
      "+---------+------------+\n",
      "|2250     |lessThan10MT|\n",
      "|15057    |lessThan10MT|\n",
      "|29089    |lessThan10MT|\n",
      "+---------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_portfolio_value_df = (\n",
    "    final_portfolio_value_df\n",
    "    .join(initial_portfolio_value_df, on = ['accountId'], how = 'outer')\n",
    "    .fillna(0)\n",
    "    .withColumn('maxPortfolioValue', F.greatest(F.col('initialPortfolioValue'), F.col('finalPortfolioValue')))\n",
    "    .withColumn('type', F.when(F.col('maxPortfolioValue') < 10, 'lessThan10MT')\n",
    "                         .when(F.col('maxPortfolioValue').between(10, 20), 'between10MTand20MT')\n",
    "                         .when(F.col('maxPortfolioValue').between(20, 50), 'between20MTand50MT')\n",
    "                         .otherwise('greaterThan50MT')\n",
    "               )\n",
    "    .select('accountId', 'type')\n",
    "    .dropDuplicates()\n",
    ")\n",
    "\n",
    "display_df(max_portfolio_value_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82e94749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2764\n",
      "+------------------+--------+-------+---------+\n",
      "|type              |date    |netCash|nAccounts|\n",
      "+------------------+--------+-------+---------+\n",
      "|between10MTand20MT|13980105|-2878.0|9837     |\n",
      "|between20MTand50MT|13980105|-2674.0|13598    |\n",
      "|greaterThan50MT   |13980105|19691.0|51532    |\n",
      "+------------------+--------+-------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dates_list = price_df.select('date').distinct().orderBy('date').rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "cash_time_series_df = (\n",
    "    adjusted_raw_flat_trade_df\n",
    "    .withColumn('netCash', F.col('cashIn') + F.col('cashOut'))\n",
    "    .join(max_portfolio_value_df, on = 'accountId', how = 'inner')\n",
    "    .groupBy('type', 'date')\n",
    "    .agg(\n",
    "        F.round((-F.sum('netCash'))).alias('netCash'),\n",
    "        F.countDistinct('accountId').alias('nAccounts')\n",
    "    )\n",
    "    .orderBy('date', 'type')\n",
    ")\n",
    "\n",
    "display_df(cash_time_series_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fe2e62b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/Administrator/Heidari_Ra/Outputs/'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4fdc6d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    cash_time_series_df.write.mode('overwrite').parquet(PATH_OUTPUT + 'cash_time_series.parquet')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385da296",
   "metadata": {},
   "source": [
    "## calculate returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2991a55a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12612568\n",
      "+---------+----------+---------+---------------------+-------------------+------+------------+\n",
      "|accountId|netCashOut|netCashIn|initialPortfolioValue|finalPortfolioValue|return|returnDecile|\n",
      "+---------+----------+---------+---------------------+-------------------+------+------------+\n",
      "|1187801  |0.0       |-0.5124  |0.0                  |0.0                |-1.0  |1           |\n",
      "|12187596 |0.0       |-4.002426|0.0                  |0.0                |-1.0  |1           |\n",
      "|2124119  |0.0       |-0.5015  |0.0                  |0.0                |-1.0  |1           |\n",
      "+---------+----------+---------+---------------------+-------------------+------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "return_df = (\n",
    "    gain_from_trade_df\n",
    "    .join(initial_portfolio_value_df, on = 'accountId', how = 'outer')\n",
    "    .join(final_portfolio_value_df, on = 'accountId', how = 'outer')\n",
    "    .fillna(0, subset = ['netCashIn', 'netCashOut', 'initialPortfolioValue', 'finalPortfolioValue'])\n",
    "    .withColumn('return', \n",
    "                ((F.col('finalPortfolioValue') + F.col('netCashOut')) / (F.col('initialPortfolioValue') + (-F.col('netCashIn')))) - 1)\n",
    "    .filter(F.col('return').isNotNull())\n",
    "    .withColumn('returnDecile', F.ntile(N_QUANTILES).over(Window.partitionBy().orderBy('return')))\n",
    ")\n",
    "\n",
    "display_df(return_df)\n",
    "# null returns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c4927024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88980"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    return_df\n",
    "    .filter(F.col('return') == 0)\n",
    "    .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e1cca050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|returnDecile|medianReturn|\n",
      "+------------+------------+\n",
      "|           1|      -0.387|\n",
      "|           2|      -0.172|\n",
      "|           3|      -0.052|\n",
      "|           4|       0.024|\n",
      "|           5|       0.123|\n",
      "|           6|       0.267|\n",
      "|           7|       0.508|\n",
      "|           8|       1.328|\n",
      "|           9|       3.712|\n",
      "|          10|       8.485|\n",
      "+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    return_df\n",
    "    .groupBy('returnDecile')\n",
    "    .agg(\n",
    "        F.round(F.expr('percentile(return, array(0.5))')[0], 3).alias('medianReturn')\n",
    "    )\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba304564",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o552.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 319.0 failed 1 times, most recent failure: Lost task 0.0 in stage 319.0 (TID 9445) (WIN-LN8FQNN8ISE executor driver): java.lang.IllegalArgumentException: Cannot grow BufferHolder by size 272947336 because the size after growing exceeds size limitation 2147483632\r\n\tat org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder.grow(BufferHolder.java:71)\r\n\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.grow(UnsafeWriter.java:63)\r\n\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.writeUnalignedBytes(UnsafeWriter.java:127)\r\n\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:118)\r\n\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:114)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.writeFields_0_2$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateResultProjection$6(AggregationIterator.scala:282)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:85)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:32)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:151)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:77)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2(ObjectHashAggregateExec.scala:107)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2$adapted(ObjectHashAggregateExec.scala:85)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:885)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:885)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor649.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.lang.IllegalArgumentException: Cannot grow BufferHolder by size 272947336 because the size after growing exceeds size limitation 2147483632\r\n\tat org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder.grow(BufferHolder.java:71)\r\n\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.grow(UnsafeWriter.java:63)\r\n\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.writeUnalignedBytes(UnsafeWriter.java:127)\r\n\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:118)\r\n\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:114)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.writeFields_0_2$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateResultProjection$6(AggregationIterator.scala:282)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:85)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:32)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:151)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:77)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2(ObjectHashAggregateExec.scala:107)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2$adapted(ObjectHashAggregateExec.scala:85)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:885)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:885)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Administrator\\Heidari_Ra\\Codes\\Codes\\Calculations.ipynb Cell 60'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Heidari_Ra/Codes/Codes/Calculations.ipynb#ch0000057?line=0'>1</a>\u001b[0m (\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Heidari_Ra/Codes/Codes/Calculations.ipynb#ch0000057?line=1'>2</a>\u001b[0m     return_df\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Heidari_Ra/Codes/Codes/Calculations.ipynb#ch0000057?line=2'>3</a>\u001b[0m     \u001b[39m.\u001b[39;49magg(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Heidari_Ra/Codes/Codes/Calculations.ipynb#ch0000057?line=3'>4</a>\u001b[0m        F\u001b[39m.\u001b[39;49mround(F\u001b[39m.\u001b[39;49mmin(\u001b[39m'\u001b[39;49m\u001b[39mreturn\u001b[39;49m\u001b[39m'\u001b[39;49m), \u001b[39m2\u001b[39;49m)\u001b[39m.\u001b[39;49malias(\u001b[39m'\u001b[39;49m\u001b[39mmin\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Heidari_Ra/Codes/Codes/Calculations.ipynb#ch0000057?line=4'>5</a>\u001b[0m         F\u001b[39m.\u001b[39;49mround(F\u001b[39m.\u001b[39;49mexpr(\u001b[39m'\u001b[39;49m\u001b[39mpercentile(return, array(0.01))\u001b[39;49m\u001b[39m'\u001b[39;49m)[\u001b[39m0\u001b[39;49m], \u001b[39m2\u001b[39;49m)\u001b[39m.\u001b[39;49malias(\u001b[39m'\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Heidari_Ra/Codes/Codes/Calculations.ipynb#ch0000057?line=5'>6</a>\u001b[0m         F\u001b[39m.\u001b[39;49mround(F\u001b[39m.\u001b[39;49mexpr(\u001b[39m'\u001b[39;49m\u001b[39mpercentile(return, array(0.1))\u001b[39;49m\u001b[39m'\u001b[39;49m)[\u001b[39m0\u001b[39;49m], \u001b[39m2\u001b[39;49m)\u001b[39m.\u001b[39;49malias(\u001b[39m'\u001b[39;49m\u001b[39m10\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Heidari_Ra/Codes/Codes/Calculations.ipynb#ch0000057?line=6'>7</a>\u001b[0m         F\u001b[39m.\u001b[39;49mround(F\u001b[39m.\u001b[39;49mexpr(\u001b[39m'\u001b[39;49m\u001b[39mpercentile(return, array(0.25))\u001b[39;49m\u001b[39m'\u001b[39;49m)[\u001b[39m0\u001b[39;49m], \u001b[39m2\u001b[39;49m)\u001b[39m.\u001b[39;49malias(\u001b[39m'\u001b[39;49m\u001b[39m25\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Heidari_Ra/Codes/Codes/Calculations.ipynb#ch0000057?line=7'>8</a>\u001b[0m         F\u001b[39m.\u001b[39;49mround(F\u001b[39m.\u001b[39;49mexpr(\u001b[39m'\u001b[39;49m\u001b[39mpercentile(return, array(0.5))\u001b[39;49m\u001b[39m'\u001b[39;49m)[\u001b[39m0\u001b[39;49m], \u001b[39m2\u001b[39;49m)\u001b[39m.\u001b[39;49malias(\u001b[39m'\u001b[39;49m\u001b[39m50\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Heidari_Ra/Codes/Codes/Calculations.ipynb#ch0000057?line=8'>9</a>\u001b[0m         F\u001b[39m.\u001b[39;49mround(F\u001b[39m.\u001b[39;49mmean(\u001b[39m'\u001b[39;49m\u001b[39mreturn\u001b[39;49m\u001b[39m'\u001b[39;49m), \u001b[39m2\u001b[39;49m)\u001b[39m.\u001b[39;49malias(\u001b[39m'\u001b[39;49m\u001b[39mmean\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Heidari_Ra/Codes/Codes/Calculations.ipynb#ch0000057?line=9'>10</a>\u001b[0m         F\u001b[39m.\u001b[39;49mround(F\u001b[39m.\u001b[39;49mexpr(\u001b[39m'\u001b[39;49m\u001b[39mpercentile(return, array(0.75))\u001b[39;49m\u001b[39m'\u001b[39;49m)[\u001b[39m0\u001b[39;49m], \u001b[39m2\u001b[39;49m)\u001b[39m.\u001b[39;49malias(\u001b[39m'\u001b[39;49m\u001b[39m75\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Heidari_Ra/Codes/Codes/Calculations.ipynb#ch0000057?line=10'>11</a>\u001b[0m         F\u001b[39m.\u001b[39;49mround(F\u001b[39m.\u001b[39;49mexpr(\u001b[39m'\u001b[39;49m\u001b[39mpercentile(return, array(0.9))\u001b[39;49m\u001b[39m'\u001b[39;49m)[\u001b[39m0\u001b[39;49m], \u001b[39m2\u001b[39;49m)\u001b[39m.\u001b[39;49malias(\u001b[39m'\u001b[39;49m\u001b[39m90\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Heidari_Ra/Codes/Codes/Calculations.ipynb#ch0000057?line=11'>12</a>\u001b[0m         F\u001b[39m.\u001b[39;49mround(F\u001b[39m.\u001b[39;49mexpr(\u001b[39m'\u001b[39;49m\u001b[39mpercentile(return, array(0.99))\u001b[39;49m\u001b[39m'\u001b[39;49m)[\u001b[39m0\u001b[39;49m], \u001b[39m2\u001b[39;49m)\u001b[39m.\u001b[39;49malias(\u001b[39m'\u001b[39;49m\u001b[39m99\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Heidari_Ra/Codes/Codes/Calculations.ipynb#ch0000057?line=12'>13</a>\u001b[0m         F\u001b[39m.\u001b[39;49mround(F\u001b[39m.\u001b[39;49mexpr(\u001b[39m'\u001b[39;49m\u001b[39mpercentile(return, array(0.999))\u001b[39;49m\u001b[39m'\u001b[39;49m)[\u001b[39m0\u001b[39;49m], \u001b[39m2\u001b[39;49m)\u001b[39m.\u001b[39;49malias(\u001b[39m'\u001b[39;49m\u001b[39m99.9\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Heidari_Ra/Codes/Codes/Calculations.ipynb#ch0000057?line=13'>14</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Heidari_Ra/Codes/Codes/Calculations.ipynb#ch0000057?line=14'>15</a>\u001b[0m     \u001b[39m.\u001b[39;49mshow()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Heidari_Ra/Codes/Codes/Calculations.ipynb#ch0000057?line=15'>16</a>\u001b[0m )\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py:484\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/lib/site-packages/pyspark/sql/dataframe.py?line=440'>441</a>\u001b[0m \u001b[39m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/lib/site-packages/pyspark/sql/dataframe.py?line=441'>442</a>\u001b[0m \n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/lib/site-packages/pyspark/sql/dataframe.py?line=442'>443</a>\u001b[0m \u001b[39m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/lib/site-packages/pyspark/sql/dataframe.py?line=480'>481</a>\u001b[0m \u001b[39m name | Bob\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/lib/site-packages/pyspark/sql/dataframe.py?line=481'>482</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/lib/site-packages/pyspark/sql/dataframe.py?line=482'>483</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(truncate, \u001b[39mbool\u001b[39m) \u001b[39mand\u001b[39;00m truncate:\n\u001b[1;32m--> <a href='file:///c%3A/ProgramData/Anaconda3/lib/site-packages/pyspark/sql/dataframe.py?line=483'>484</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mshowString(n, \u001b[39m20\u001b[39;49m, vertical))\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/lib/site-packages/pyspark/sql/dataframe.py?line=484'>485</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/lib/site-packages/pyspark/sql/dataframe.py?line=485'>486</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jdf\u001b[39m.\u001b[39mshowString(n, \u001b[39mint\u001b[39m(truncate), vertical))\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py:1309\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/lib/site-packages/py4j/java_gateway.py?line=1302'>1303</a>\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/lib/site-packages/py4j/java_gateway.py?line=1303'>1304</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/lib/site-packages/py4j/java_gateway.py?line=1304'>1305</a>\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/lib/site-packages/py4j/java_gateway.py?line=1305'>1306</a>\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/lib/site-packages/py4j/java_gateway.py?line=1307'>1308</a>\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> <a href='file:///c%3A/ProgramData/Anaconda3/lib/site-packages/py4j/java_gateway.py?line=1308'>1309</a>\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/lib/site-packages/py4j/java_gateway.py?line=1309'>1310</a>\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/lib/site-packages/py4j/java_gateway.py?line=1311'>1312</a>\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/lib/site-packages/py4j/java_gateway.py?line=1312'>1313</a>\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/lib/site-packages/pyspark/sql/utils.py?line=108'>109</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/lib/site-packages/pyspark/sql/utils.py?line=109'>110</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/ProgramData/Anaconda3/lib/site-packages/pyspark/sql/utils.py?line=110'>111</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/lib/site-packages/pyspark/sql/utils.py?line=111'>112</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m py4j\u001b[39m.\u001b[39mprotocol\u001b[39m.\u001b[39mPy4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/lib/site-packages/pyspark/sql/utils.py?line=112'>113</a>\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/lib/site-packages/py4j/protocol.py?line=323'>324</a>\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/lib/site-packages/py4j/protocol.py?line=324'>325</a>\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> <a href='file:///c%3A/ProgramData/Anaconda3/lib/site-packages/py4j/protocol.py?line=325'>326</a>\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/lib/site-packages/py4j/protocol.py?line=326'>327</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/lib/site-packages/py4j/protocol.py?line=327'>328</a>\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/lib/site-packages/py4j/protocol.py?line=328'>329</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/lib/site-packages/py4j/protocol.py?line=329'>330</a>\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/lib/site-packages/py4j/protocol.py?line=330'>331</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/lib/site-packages/py4j/protocol.py?line=331'>332</a>\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o552.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 319.0 failed 1 times, most recent failure: Lost task 0.0 in stage 319.0 (TID 9445) (WIN-LN8FQNN8ISE executor driver): java.lang.IllegalArgumentException: Cannot grow BufferHolder by size 272947336 because the size after growing exceeds size limitation 2147483632\r\n\tat org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder.grow(BufferHolder.java:71)\r\n\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.grow(UnsafeWriter.java:63)\r\n\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.writeUnalignedBytes(UnsafeWriter.java:127)\r\n\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:118)\r\n\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:114)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.writeFields_0_2$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateResultProjection$6(AggregationIterator.scala:282)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:85)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:32)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:151)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:77)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2(ObjectHashAggregateExec.scala:107)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2$adapted(ObjectHashAggregateExec.scala:85)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:885)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:885)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor649.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.lang.IllegalArgumentException: Cannot grow BufferHolder by size 272947336 because the size after growing exceeds size limitation 2147483632\r\n\tat org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder.grow(BufferHolder.java:71)\r\n\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.grow(UnsafeWriter.java:63)\r\n\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.writeUnalignedBytes(UnsafeWriter.java:127)\r\n\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:118)\r\n\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:114)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.writeFields_0_2$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateResultProjection$6(AggregationIterator.scala:282)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:85)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:32)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:151)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:77)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2(ObjectHashAggregateExec.scala:107)\r\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2$adapted(ObjectHashAggregateExec.scala:85)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:885)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:885)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    return_df\n",
    "    .agg(\n",
    "       F.round(F.min('return'), 2).alias('min'),\n",
    "        F.round(F.expr('percentile(return, array(0.01))')[0], 2).alias('1%'),\n",
    "        F.round(F.expr('percentile(return, array(0.1))')[0], 2).alias('10%'),\n",
    "        F.round(F.expr('percentile(return, array(0.25))')[0], 2).alias('25%'),\n",
    "        F.round(F.expr('percentile(return, array(0.5))')[0], 2).alias('50%'),\n",
    "        F.round(F.mean('return'), 2).alias('mean'),\n",
    "        F.round(F.expr('percentile(return, array(0.75))')[0], 2).alias('75%'),\n",
    "        F.round(F.expr('percentile(return, array(0.9))')[0], 2).alias('90%'),\n",
    "        F.round(F.expr('percentile(return, array(0.99))')[0], 2).alias('99%'),\n",
    "        F.round(F.expr('percentile(return, array(0.999))')[0], 2).alias('99.9%'),\n",
    "    )\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7359d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    return_df.write.mode('overwrite').parquet(PATH_OUTPUT + '/return_output.parquet')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6f543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return_df.filter(F.col('return') == return_df.select('return').rdd.max()[0]).show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9407ea96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\n",
    "#     price_df\n",
    "#     .filter((F.col(\"symbol\") == 'ودانا')&(F.col(\"date\") > 13980305))\n",
    "#     .orderBy(\"date\")\n",
    "#     .select(\n",
    "#         F.col(\"date\"),\n",
    "#         F.col(\"close_price\"),\n",
    "#         F.col(\"close_price_adjusted\"),\n",
    "#     )\n",
    "#     .show(20,False)\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9304442a",
   "metadata": {},
   "source": [
    "### final portfolio value output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a77650",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_final_portfolio_value = (\n",
    "    final_portfolio_value_df\n",
    "    .join(return_df.select('accountId', 'return'), on = 'accountId')\n",
    "    .withColumn('finalPortfolioValueDecile', F.ntile(N_QUANTILES).over(Window.partitionBy().orderBy('finalPortfolioValue')))\n",
    ")\n",
    "\n",
    "display_df(output_final_portfolio_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdff893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    output_final_portfolio_value\n",
    "    .groupBy('finalPortfolioValueDecile')\n",
    "    .agg(\n",
    "        F.round(F.expr('percentile(finalPortfolioValue, array(0.5))')[0], 3).alias('medianFinalPortfolioValue'),\n",
    "        F.round(F.mean('return'), 2).alias('meanReturn'),\n",
    "        F.round(F.expr('percentile(return, array(0.5))')[0], 3).alias('medianReturn')\n",
    "    )\n",
    "    .orderBy('finalPortfolioValueDecile')\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bce316",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    output_final_portfolio_value.write.mode('overwrite').parquet(PATH_OUTPUT + '/final_portfolio_output.parquet')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2173cf42",
   "metadata": {},
   "source": [
    "### initial portfolio value output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5e118a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_initial_portfolio_value = (\n",
    "    initial_portfolio_value_df\n",
    "    .join(return_df.select('accountId', 'return'), on = 'accountId')\n",
    "    .withColumn('initialPortfolioValueDecile', F.ntile(N_QUANTILES).over(Window.partitionBy().orderBy('initialPortfolioValue')))\n",
    ")\n",
    "\n",
    "display_df(output_initial_portfolio_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba26784",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    output_initial_portfolio_value\n",
    "    .groupBy('initialPortfolioValueDecile')\n",
    "    .agg(\n",
    "        F.round(F.expr('percentile(initialPortfolioValue, array(0.5))')[0], 3).alias('medianInitialPortfolioValue'),\n",
    "        F.round(F.mean('return'), 2).alias('meanReturn'),\n",
    "        F.round(F.expr('percentile(return, array(0.5))')[0], 5).alias('medianReturn')\n",
    "    )\n",
    "    .orderBy('initialPortfolioValueDecile')\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c42db78",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    output_initial_portfolio_value.write.mode('overwrite').parquet(PATH_OUTPUT + '/inital_portfolio_output.parquet')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4358eef",
   "metadata": {},
   "source": [
    "### calculate frequency of trades and active days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc91189f",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_days_df = (\n",
    "    adjusted_raw_flat_trade_df\n",
    "    .groupBy('accountId', 'date')\n",
    "    .agg(\n",
    "        F.sum('cashIn').alias('netCashIn'),\n",
    "        F.sum('cashOut').alias('netCashOut')\n",
    "    )\n",
    "    .withColumn('netCash', F.col('netCashIn') + F.col('netCashOut'))\n",
    "    .groupBy('accountId')\n",
    "    .agg(\n",
    "        F.count(F.when(F.col('netCash') < 0, F.lit(1))).alias('nBuyDays'),\n",
    "        F.count(F.when(F.col('netCash') > 0, F.lit(1))).alias('nSellDays')\n",
    "    )\n",
    "    .fillna(0, subset = ['nBuyDays', 'nSellDays'])\n",
    ")\n",
    "\n",
    "display_df(active_days_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a9d430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# buy_trade_df = (\n",
    "#     trade_df\n",
    "#         .select(\n",
    "#         'date',\n",
    "#         'symbol',\n",
    "#         F.col('buyerAccountId').alias('accountId'),\n",
    "#         'nTradeShares',\n",
    "#         (-F.col('tradeSettlementValue')).alias('settlementValue'),\n",
    "#         )\n",
    "# )\n",
    "\n",
    "# sell_trade_df = (\n",
    "#     trade_df\n",
    "#         .select(\n",
    "#             'date',\n",
    "#             'symbol',\n",
    "#             F.col('sellerAccountId').alias('accountId'),\n",
    "#             (-F.col('nTradeShares')).alias('nTradeShares'),\n",
    "#             F.col('tradeSettlementValue').alias('settlementValue')\n",
    "#         )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2e1e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trade_kpi_df = (\n",
    "#     buy_trade_df\n",
    "#     .union(sell_trade_df)\n",
    "#     .groupBy('accountId')\n",
    "#     .agg(\n",
    "#         F.count(F.lit(1)).alias('tradeFrequency'),\n",
    "#         F.mean(F.abs('settlementValue')).alias('meanTradeValue'),\n",
    "#         F.sum('settlementValue').alias('netSumTradeValue'),\n",
    "#         F.sum(F.abs('settlementValue')).alias('absSumTradeValue'),\n",
    "#         F.countDistinct('date').alias('activeDays'),\n",
    "#     )\n",
    "#     .join(active_days_df, on = 'accountId')\n",
    "# )\n",
    "\n",
    "# display_df(trade_kpi_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34facbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\n",
    "#     trade_kpi_df\n",
    "#     .agg(\n",
    "#         F.round(F.expr('percentile(tradeFrequency, array(0.25))')[0], 2).alias('25% percentile'),\n",
    "#         F.round(F.expr('percentile(tradeFrequency, array(0.5))')[0], 2).alias('50% percentile'),\n",
    "#         F.round(F.mean('tradeFrequency'), 2).alias('mean'),\n",
    "#         F.round(F.expr('percentile(tradeFrequency, array(0.75))')[0], 2).alias('75% percentile'),\n",
    "#         F.round(F.expr('percentile(tradeFrequency, array(0.9))')[0], 2).alias('90% percentile'),\n",
    "#         F.round(F.expr('percentile(tradeFrequency, array(0.99))')[0], 2).alias('99% percentile'),\n",
    "#         F.round(F.expr('percentile(tradeFrequency, array(0.999))')[0], 2).alias('99.9% percentile'),\n",
    "#     )\n",
    "#     .show()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4932f062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\n",
    "#     trade_kpi_df\n",
    "#     .agg(\n",
    "#         F.round(F.expr('percentile(activeDays, array(0.25))')[0], 2).alias('25% percentile'),\n",
    "#         F.round(F.expr('percentile(activeDays, array(0.5))')[0], 2).alias('50% percentile'),\n",
    "#         F.round(F.mean('activeDays'), 2).alias('mean'),\n",
    "#         F.round(F.expr('percentile(activeDays, array(0.75))')[0], 2).alias('75% percentile'),\n",
    "#         F.round(F.expr('percentile(activeDays, array(0.9))')[0], 2).alias('90% percentile'),\n",
    "#         F.round(F.expr('percentile(activeDays, array(0.99))')[0], 2).alias('99% percentile'),\n",
    "#         F.round(F.expr('percentile(activeDays, array(0.999))')[0], 2).alias('99.9% percentile'),\n",
    "#     )\n",
    "#     .show()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a55f696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\n",
    "#     trade_kpi_df\n",
    "#     .agg(\n",
    "#         F.round(F.expr('percentile(nBuyDays, array(0.25))')[0], 2).alias('25% percentile'),\n",
    "#         F.round(F.expr('percentile(nBuyDays, array(0.5))')[0], 2).alias('50% percentile'),\n",
    "#         F.round(F.mean('nBuyDays'), 2).alias('mean'),\n",
    "#         F.round(F.expr('percentile(nBuyDays, array(0.75))')[0], 2).alias('75% percentile'),\n",
    "#         F.round(F.expr('percentile(nBuyDays, array(0.9))')[0], 2).alias('90% percentile'),\n",
    "#         F.round(F.expr('percentile(nBuyDays, array(0.99))')[0], 2).alias('99% percentile'),\n",
    "#         F.round(F.expr('percentile(nBuyDays, array(0.999))')[0], 2).alias('99.9% percentile'),\n",
    "#     )\n",
    "#     .show()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2270b6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\n",
    "#     trade_kpi_df\n",
    "#     .agg(\n",
    "#         F.round(F.expr('percentile(nSellDays, array(0.25))')[0], 2).alias('25% percentile'),\n",
    "#         F.round(F.expr('percentile(nSellDays, array(0.5))')[0], 2).alias('50% percentile'),\n",
    "#         F.round(F.mean('nSellDays'), 2).alias('mean'),\n",
    "#         F.round(F.expr('percentile(nSellDays, array(0.75))')[0], 2).alias('75% percentile'),\n",
    "#         F.round(F.expr('percentile(nSellDays, array(0.9))')[0], 2).alias('90% percentile'),\n",
    "#         F.round(F.expr('percentile(nSellDays, array(0.99))')[0], 2).alias('99% percentile'),\n",
    "#         F.round(F.expr('percentile(nSellDays, array(0.999))')[0], 2).alias('99.9% percentile'),\n",
    "#     )\n",
    "#     .show()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69836438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(trade_kpi_df.count() - trade_kpi_df.dropna().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac8cec4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trade_output_df = (\n",
    "#     trade_kpi_df\n",
    "#     .join(return_df.select('accountId', 'return').dropDuplicates(), on = ['accountId'])\n",
    "#     .dropna()\n",
    "#     .withColumn('tradeFrequencyDecile', F.ntile(N_QUANTILES).over(Window.partitionBy().orderBy('tradeFrequency')))\n",
    "#     .withColumn('meanTradeValueDecile', F.ntile(N_QUANTILES).over(Window.partitionBy().orderBy('meanTradeValue')))\n",
    "#     .withColumn('netSumTradeValueDecile', F.ntile(N_QUANTILES).over(Window.partitionBy().orderBy('netSumTradeValue')))\n",
    "#     .withColumn('absSumTradeValueDecile', F.ntile(N_QUANTILES).over(Window.partitionBy().orderBy('absSumTradeValue')))\n",
    "#     .withColumn('activeDaysDecile', F.ntile(N_QUANTILES).over(Window.partitionBy().orderBy('activeDays')))\n",
    "#     .withColumn('nBuyDaysDecile', F.ntile(N_QUANTILES).over(Window.partitionBy().orderBy('nBuyDays')))\n",
    "#     .withColumn('nSellDaysDecile', F.ntile(N_QUANTILES).over(Window.partitionBy().orderBy('nSellDays')))\n",
    "# )\n",
    "\n",
    "# display_df(trade_output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535e3153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\n",
    "#     trade_output_df\n",
    "#     .agg(\n",
    "#         F.round(F.expr('percentile(return, array(0.5))')[0], 3).alias('medianReturn')\n",
    "#     )\n",
    "#     .show()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178daac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\n",
    "#     trade_output_df\n",
    "#     .groupBy('tradeFrequencyDecile')\n",
    "#     .agg(\n",
    "#         F.round(F.expr('percentile(return, array(0.5))')[0], 3).alias('medianReturn')\n",
    "#     )\n",
    "#     .orderBy('tradeFrequencyDecile')\n",
    "#     .show()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530f71a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\n",
    "#     trade_output_df.write.mode('overwrite').parquet(PATH_OUTPUT + '/trade_output.parquet')\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f061bd",
   "metadata": {},
   "source": [
    "### identify block holders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723c64d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bh_df = (\n",
    "    daily_portfolio_df\n",
    "    .select('date', 'symbol', 'accountId', 'heldShares')\n",
    "    .join(price_df.select('date', 'symbol', 'shrout'), on = ['date', 'symbol'])\n",
    "    .withColumn('ownership', F.col('heldShares') / F.col('shrout'))\n",
    "    .filter( (F.col('ownership') >= 0.01) & F.col('ownership').isNotNull() )\n",
    "    .select('accountId')\n",
    "    .distinct()\n",
    "    .withColumn('isBH', F.lit(1))\n",
    ")\n",
    "\n",
    "display_df(bh_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fc40ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "bh_output_df = (\n",
    "    return_df\n",
    "    .select('accountId', 'return')\n",
    "    .dropna()\n",
    "    .join(bh_df, on = 'accountId', how = 'left')\n",
    "    .fillna(0, 'isBH')\n",
    ")\n",
    "\n",
    "display_df(bh_output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5f1ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    bh_output_df\n",
    "    .groupBy('isBH')\n",
    "    .agg(\n",
    "        F.round(F.expr('percentile(return, array(0.5))')[0], 3).alias('medianTradeFrequency')\n",
    "    )\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5049fb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    bh_output_df.write.mode('overwrite').parquet(PATH_OUTPUT + '/bhOutput.parquet')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c60934",
   "metadata": {},
   "source": [
    "### number of stocks within initial portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab6bed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_stocks_within_initial_portfolio_df = (\n",
    "#     portfolio_df\n",
    "#     .groupBy('accountId')\n",
    "#     .agg(\n",
    "#         F.count(F.lit(1)).alias('nStocksWithinInitialPortfolio')\n",
    "#     )\n",
    "#     .dropna()\n",
    "# )\n",
    "\n",
    "# display_df(n_stocks_within_initial_portfolio_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2e38a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_stocks_within_initial_portfolio_output_df = (\n",
    "#     return_df\n",
    "#     .select('accountId', 'return')\n",
    "#     .dropna()\n",
    "#     .join(n_stocks_within_initial_portfolio_df, on = 'accountId', how = 'inner')\n",
    "#     .withColumn('nStocksWithinInitialPortfolioDecile', F.ntile(N_QUANTILES).over(Window.partitionBy().orderBy('nStocksWithinInitialPortfolio')))\n",
    "# )\n",
    "\n",
    "# display_df(n_stocks_within_initial_portfolio_output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864f21c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\n",
    "#     n_stocks_within_initial_portfolio_output_df\n",
    "#     .groupBy('nStocksWithinInitialPortfolioDecile')\n",
    "#     .agg(\n",
    "#         F.round(F.expr('percentile(return, array(0.5))')[0], 3).alias('medianReturn')\n",
    "#     )\n",
    "#     .orderBy('nStocksWithinInitialPortfolioDecile')\n",
    "#     .show()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8613fe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\n",
    "#     n_stocks_within_initial_portfolio_output_df.write.mode('overwrite').parquet(PATH_OUTPUT + '/n_initial_portfolio.parquet')\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7933d8",
   "metadata": {},
   "source": [
    "### number of stocks within final portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f432cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_stocks_within_final_portfolio_df = (\n",
    "    daily_portfolio_df\n",
    "    .withColumn('rowNumber', F.row_number().over(Window.partitionBy('accountId', 'symbol').orderBy('date')))\n",
    "    .withColumn('maxRowNumber', F.max('rowNumber').over(Window.partitionBy('accountId', 'symbol')))\n",
    "    .filter(F.col('rowNumber') == F.col('maxRowNumber'))\n",
    "    .filter(F.col('heldShares') > 0)\n",
    "    .withColumn('date', F.lit(MAX_PRICE_DATE))\n",
    "    .join(price_df.select('date', 'symbol', 'close_price'), on = ['date', 'symbol'], how = 'left')\n",
    "    .dropna(subset = ['close_price'])\n",
    "    .groupBy('accountId')\n",
    "    .agg(\n",
    "        F.countDistinct('symbol').alias('nStocksWithinFinalPortfolio')\n",
    "    )   \n",
    ")\n",
    "\n",
    "display_df(n_stocks_within_final_portfolio_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a10563",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_stocks_within_final_portfolio_output_df = (\n",
    "    return_df\n",
    "    .select('accountId', 'return')\n",
    "    .dropna()\n",
    "    .join(n_stocks_within_final_portfolio_df, on = 'accountId', how = 'inner')\n",
    "    .withColumn('nStocksWithinFinalPortfolioDecile', F.ntile(N_QUANTILES).over(Window.partitionBy().orderBy('nStocksWithinFinalPortfolio')))\n",
    ")\n",
    "\n",
    "display_df(n_stocks_within_final_portfolio_output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932ef467",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    n_stocks_within_final_portfolio_output_df\n",
    "    .groupBy('nStocksWithinFinalPortfolioDecile')\n",
    "    .agg(\n",
    "        F.round(F.expr('percentile(return, array(0.5))')[0], 3).alias('medianReturn')\n",
    "    )\n",
    "    .orderBy('nStocksWithinFinalPortfolioDecile')\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ee5b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    n_stocks_within_final_portfolio_output_df.write.mode('overwrite').parquet(PATH_OUTPUT + '/n_final_portfolio.parquet')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a87605",
   "metadata": {},
   "source": [
    "### turnover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ab037f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# turnover_df = (\n",
    "#     trade_kpi_df\n",
    "#     .join(final_portfolio_value_df, on =['accountId'], how = 'right')\n",
    "#     .fillna(0, subset = ['absSumTradeValue'])\n",
    "#     .withColumn('turnover', F.col('absSumTradeValue') / F.col('finalPortfolioValue'))\n",
    "#     .join(return_df.select('accountId', 'return'), on = 'accountId')\n",
    "#     .withColumn('turnoverDecile', F.ntile(N_QUANTILES).over(Window.partitionBy().orderBy(F.col('turnover'))))\n",
    "#     .select(\n",
    "#         'accountId',\n",
    "#         'turnover',\n",
    "#         'turnoverDecile',\n",
    "#         'return'\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# display_df(turnover_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eee8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\n",
    "#     turnover_df\n",
    "#     .groupBy('turnoverDecile')\n",
    "#     .agg(\n",
    "#         F.round(F.expr('percentile(return, array(0.5))')[0], 3).alias('medianReturn')\n",
    "#     )\n",
    "#     .orderBy('turnoverDecile')\n",
    "#     .show()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a07c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\n",
    "#     turnover_df.write.mode('overwrite').parquet(PATH_OUTPUT + '/turnover.parquet')\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0658b581",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
